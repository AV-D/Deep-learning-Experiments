{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Step1. Run the demo and train a model on the original German-to-English training set.**"
      ],
      "metadata": {
        "id": "JbbnbdKuJ__Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "b0uDztHr8mZp"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZRJwbB88mZr"
      },
      "source": [
        "*Data Sourcing and Processing*\n",
        "============================\n",
        "\n",
        "We will use\n",
        "[Multi30k dataset from torchtext\n",
        "library](https://pytorch.org/text/stable/datasets.html#multi30k) that\n",
        "yields a pair of source-target raw sentences.\n",
        "\n",
        "To access torchtext datasets, please install torchdata following\n",
        "instructions at <https://github.com/pytorch/data>.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eHLhJBS58mZr"
      },
      "outputs": [],
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import multi30k, Multi30k\n",
        "from typing import Iterable, List\n",
        "\n",
        "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
        "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
        "\n",
        "SRC_LANGUAGE = 'de'\n",
        "TGT_LANGUAGE = 'en'\n",
        "\n",
        "# Place-holders\n",
        "token_transform = {}\n",
        "vocab_transform = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2RfTA2FBEXJZ"
      },
      "outputs": [],
      "source": [
        "!pip install portalocker>=2.0.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchdata"
      ],
      "metadata": {
        "id": "k7U81_APKb6i"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import portalocker"
      ],
      "metadata": {
        "id": "2xen53dXK-Bs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !python -m spacy download en_core_web_sm\n",
        "# !python -m spacy download de_core_news_sm"
      ],
      "metadata": {
        "id": "LKmtz7ulKv-0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8oGE9Erf8mZs"
      },
      "outputs": [],
      "source": [
        "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
        "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "\n",
        "# helper function to yield list of tokens\n",
        "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
        "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
        "\n",
        "    for data_sample in data_iter:\n",
        "        yield token_transform[language](data_sample[language_index[language]])\n",
        "\n",
        "# Define special symbols and indices\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "\"\"\"\n",
        "Purpose: Defines special tokens used in the vocabulary for machine learning tasks with text data.\n",
        "Special Tokens:\n",
        "<unk>: \"Unknown\" token (represents words not in the vocabulary)\n",
        "<pad>: Padding token (to make sequences the same length)\n",
        "<bos>: \"Beginning of Sequence\"\n",
        "<eos>: \"End of Sequence\"\n",
        "\"\"\"\n",
        "\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    # Training data Iterator\n",
        "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    # Create torchtext's Vocab object\n",
        "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
        "                                                    min_freq=1,\n",
        "                                                    specials=special_symbols,\n",
        "                                                    special_first=True)\n",
        "\n",
        "# Set ``UNK_IDX`` as the default index. This index is returned when the token is not found.\n",
        "# If not set, it throws ``RuntimeError`` when the queried token is not found in the Vocabulary.\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "  vocab_transform[ln].set_default_index(UNK_IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "H75f_sDH8mZs"
      },
      "outputs": [],
      "source": [
        "from torch import Tensor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Transformer\n",
        "import math\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_size: int,\n",
        "                 dropout: float,\n",
        "                 maxlen: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
        "\n",
        "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "# Seq2Seq Network\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_encoder_layers: int,\n",
        "                 num_decoder_layers: int,\n",
        "                 emb_size: int,\n",
        "                 nhead: int,\n",
        "                 src_vocab_size: int,\n",
        "                 tgt_vocab_size: int,\n",
        "                 dim_feedforward: int = 512,\n",
        "                 dropout: float = 0.1):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        self.transformer = Transformer(d_model=emb_size,\n",
        "                                       nhead=nhead,\n",
        "                                       num_encoder_layers=num_encoder_layers,\n",
        "                                       num_decoder_layers=num_decoder_layers,\n",
        "                                       dim_feedforward=dim_feedforward,\n",
        "                                       dropout=dropout)\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            emb_size, dropout=dropout)\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor,\n",
        "                trg: Tensor,\n",
        "                src_mask: Tensor,\n",
        "                tgt_mask: Tensor,\n",
        "                src_padding_mask: Tensor,\n",
        "                tgt_padding_mask: Tensor,\n",
        "                memory_key_padding_mask: Tensor):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
        "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        return self.transformer.encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer.decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "R9k4gwX38mZs"
      },
      "outputs": [],
      "source": [
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "    src_seq_len = src.shape[0]\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
        "\n",
        "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-J3A-dBW8mZt"
      },
      "source": [
        "We now define the parameters of our model and instantiate the same.\n",
        "Below, we also define our loss function which is the cross-entropy loss\n",
        "and the optimizer used for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xl-3VhHc8mZt",
        "outputId": "91618f89-1026-4773-b9bb-5202c9dec2c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
        "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
        "EMB_SIZE = 512\n",
        "NHEAD = 8\n",
        "FFN_HID_DIM = 512\n",
        "BATCH_SIZE = 128\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "NUM_DECODER_LAYERS = 3\n",
        "\n",
        "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
        "\n",
        "for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "transformer = transformer.to(DEVICE)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT14pfqm8mZt"
      },
      "source": [
        "Collation\n",
        "=========\n",
        "we define our collate function that converts a\n",
        "batch of raw strings into batch tensors that can be fed directly into\n",
        "our model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "qSnmfKDK8mZt"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# helper function to club together sequential operations\n",
        "def sequential_transforms(*transforms):\n",
        "    def func(txt_input):\n",
        "        for transform in transforms:\n",
        "            txt_input = transform(txt_input)\n",
        "        return txt_input\n",
        "    return func\n",
        "\n",
        "# function to add BOS/EOS and create tensor for input sequence indices\n",
        "def tensor_transform(token_ids: List[int]):\n",
        "    return torch.cat((torch.tensor([BOS_IDX]),\n",
        "                      torch.tensor(token_ids),\n",
        "                      torch.tensor([EOS_IDX])))\n",
        "\n",
        "# ``src`` and ``tgt`` language text transforms to convert raw strings into tensors indices\n",
        "text_transform = {}\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
        "                                               vocab_transform[ln], #Numericalization\n",
        "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
        "\n",
        "\n",
        "# function to collate data samples into batch tensors\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
        "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
        "    return src_batch, tgt_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZLSiBGg8mZt"
      },
      "source": [
        "We now define training and evaluation loop that will be called for each\n",
        "epoch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "SWpbimjf8mZt"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train_epoch(model, optimizer):\n",
        "    model.train()\n",
        "    losses = 0\n",
        "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "    for src, tgt in train_dataloader:\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(list(train_dataloader))\n",
        "\n",
        "\n",
        "def evaluate(model):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "\n",
        "    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "    for src, tgt in val_dataloader:\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(list(val_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3IqNfei8mZt",
        "outputId": "9b48fe95-2adb-4668-a04e-838dcb8a6d58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5109: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/combining.py:337: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
            "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Train loss: 5.344, Val loss: 4.103, Epoch time = 114.758s\n",
            "Epoch: 2, Train loss: 3.759, Val loss: 3.310, Epoch time = 113.627s\n",
            "Epoch: 3, Train loss: 3.159, Val loss: 2.891, Epoch time = 114.837s\n",
            "Epoch: 4, Train loss: 2.768, Val loss: 2.643, Epoch time = 115.061s\n",
            "Epoch: 5, Train loss: 2.479, Val loss: 2.444, Epoch time = 115.090s\n",
            "Epoch: 6, Train loss: 2.252, Val loss: 2.311, Epoch time = 116.286s\n",
            "Epoch: 7, Train loss: 2.062, Val loss: 2.198, Epoch time = 115.066s\n",
            "Epoch: 8, Train loss: 1.899, Val loss: 2.110, Epoch time = 114.630s\n",
            "Epoch: 9, Train loss: 1.756, Val loss: 2.062, Epoch time = 115.050s\n",
            "Epoch: 10, Train loss: 1.636, Val loss: 2.018, Epoch time = 115.461s\n",
            "Epoch: 11, Train loss: 1.524, Val loss: 1.974, Epoch time = 115.160s\n",
            "Epoch: 12, Train loss: 1.423, Val loss: 1.956, Epoch time = 114.551s\n",
            "Epoch: 13, Train loss: 1.333, Val loss: 1.941, Epoch time = 116.141s\n",
            "Epoch: 14, Train loss: 1.253, Val loss: 1.924, Epoch time = 117.210s\n",
            "Epoch: 15, Train loss: 1.177, Val loss: 1.905, Epoch time = 117.061s\n",
            "Epoch: 16, Train loss: 1.105, Val loss: 1.901, Epoch time = 114.374s\n",
            "Epoch: 17, Train loss: 1.036, Val loss: 1.908, Epoch time = 117.558s\n",
            "Epoch: 18, Train loss: 0.972, Val loss: 1.905, Epoch time = 117.196s\n",
            "Epoch: 19, Train loss: 0.916, Val loss: 1.924, Epoch time = 116.272s\n",
            "Epoch: 20, Train loss: 0.865, Val loss: 1.937, Epoch time = 115.530s\n",
            "Epoch: 21, Train loss: 0.815, Val loss: 1.948, Epoch time = 114.853s\n",
            "Epoch: 22, Train loss: 0.768, Val loss: 1.959, Epoch time = 117.046s\n",
            "Epoch: 23, Train loss: 0.723, Val loss: 1.973, Epoch time = 116.390s\n",
            "Epoch: 24, Train loss: 0.681, Val loss: 1.963, Epoch time = 115.145s\n",
            "Epoch: 25, Train loss: 0.639, Val loss: 1.974, Epoch time = 116.800s\n"
          ]
        }
      ],
      "source": [
        "from timeit import default_timer as timer\n",
        "NUM_EPOCHS = 25\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    start_time = timer()\n",
        "    train_loss = train_epoch(transformer, optimizer)\n",
        "    end_time = timer()\n",
        "    val_loss = evaluate(transformer)\n",
        "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
        "\n",
        "\n",
        "# function to generate output sequence using greedy algorithm\n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(DEVICE)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                    .type(torch.bool)).to(DEVICE)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "\n",
        "# actual function to translate input sentence into target language\n",
        "def translate(model: torch.nn.Module, src_sentence: str):\n",
        "    model.eval()\n",
        "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = greedy_decode(\n",
        "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
        "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRNpmM2D8mZt",
        "outputId": "5c4023a1-eb29-4eec-ba9a-076436135ff1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " A group of people standing in front an igloo . \n"
          ]
        }
      ],
      "source": [
        "print(translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu.\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(translate(transformer, \"Das Auto steht vor dem Haus.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xa-YKNF3QNJO",
        "outputId": "fd49af01-a1a5-4291-9c38-49a54f06d5a2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The car is standing in front of the house . \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(translate(transformer, \"Die Katze liegt auf dem Sofa.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6UhqqQwQf3Q",
        "outputId": "5acbf8b2-8d0a-4879-e983-0a51fcd8d34f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The cat is laying on the couch . \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step3. Insert novel sentences into your English-to-German model. Take the output and feed it to the original German-to-English model. Observe and report qualitatively on the results.**"
      ],
      "metadata": {
        "id": "SS-6yrFqISeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(translate(transformer, \"Ein Tischler fängt einen Wettbewerb für eine Aufgabe.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XE8XG3iyuZ2",
        "outputId": "8934dfb0-ed76-4774-c5eb-84a87ac5d4e3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " There is a male athlete catching an open task for an task . \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(translate(transformer, \" Eine Gruppe von Menschen steht vor einem Iglu . \"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skJsy_TOyuVR",
        "outputId": "03d4111d-e03d-410a-fc0d-4e64340f65a0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " A group of people standing in front an igloo . \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(translate(transformer, \"  Eine Gruppe von Personen steht vor einem Labor.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R906vfunyuPd",
        "outputId": "a13c50c5-ed40-4da1-f35a-03a432c8dfed"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " A group of people stand in front of a lab . \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(translate(transformer, \" Der freien Himmel scheint hell erleuchteten Himmel auf den klaren Himmel.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vim0akOX_pko",
        "outputId": "19870826-5fd6-4649-a2d3-6837adb58a3a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " A professional sky - deep lit sky in the clear blue sky . \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(translate(transformer, \" Kinder spielen im Park .\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7Lxz-qj_paS",
        "outputId": "9a4c670a-b827-4aee-d4a2-21d412d81536"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " There are kids playing in the park . \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "# Data initialization\n",
        "data = [\n",
        "    (\"A group of people stand in front of an igloo.\", \"A group of people standing in front an igloo.\"),\n",
        "    (\"The car is in front of the house\", \"The car is standing in front of the house.\"),\n",
        "    (\"The cat is lying on the sofa.\", \"The cat is laying on the couch.\"),\n",
        "    (\"A carpenter starts a competition for a task.\", \"There is a male athlete catching an open task for an task.\"),\n",
        "    (\"A group of people stand in front of a laboratory.\", \"A group of people stand in front of a lab.\"),\n",
        "    (\"The open sky shines brightly lit sky on the clear sky.\", \"A professional sky - deep lit sky in the clear blue sky.\"),\n",
        "    (\"Children play in the park.\", \"There are kids playing in the park.\")\n",
        "]\n",
        "\n",
        "# Function to calculate BLEU scores\n",
        "def calculate_bleu(data):\n",
        "    scores = []\n",
        "    for actual, translated in data:\n",
        "        reference = [actual.split()]\n",
        "        candidate = translated.split()\n",
        "        score = round(sentence_bleu(reference, candidate),2)\n",
        "        scores.append(score)\n",
        "        print(f\"Actual: {actual}\\nTranslated: {translated}\\nBLEU score: {score}\\n\")\n",
        "    return scores\n",
        "\n",
        "# Calculate and print BLEU scores\n",
        "bleu_scores = calculate_bleu(data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjW7qq2OEIIt",
        "outputId": "81f3005c-4de6-4726-cb85-341d6dd72e41"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual: A group of people stand in front of an igloo.\n",
            "Translated: A group of people standing in front an igloo.\n",
            "BLEU score: 0.36\n",
            "\n",
            "Actual: The car is in front of the house\n",
            "Translated: The car is standing in front of the house.\n",
            "BLEU score: 0.43\n",
            "\n",
            "Actual: The cat is lying on the sofa.\n",
            "Translated: The cat is laying on the couch.\n",
            "BLEU score: 0.0\n",
            "\n",
            "Actual: A carpenter starts a competition for a task.\n",
            "Translated: There is a male athlete catching an open task for an task.\n",
            "BLEU score: 0.0\n",
            "\n",
            "Actual: A group of people stand in front of a laboratory.\n",
            "Translated: A group of people stand in front of a lab.\n",
            "BLEU score: 0.88\n",
            "\n",
            "Actual: The open sky shines brightly lit sky on the clear sky.\n",
            "Translated: A professional sky - deep lit sky in the clear blue sky.\n",
            "BLEU score: 0.0\n",
            "\n",
            "Actual: Children play in the park.\n",
            "Translated: There are kids playing in the park.\n",
            "BLEU score: 0.0\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| SNO | German                                           | Actual English                                  | Translated English                                     |\n",
        "|-----|--------------------------------------------------|-------------------------------------------------|--------------------------------------------------------|\n",
        "| 1   | Eine Gruppe von Menschen steht vor einem Iglu. | A group of people stand in front of an igloo. | A group of people standing in front an igloo .        |\n",
        "| 2   | Das Auto steht vor dem Haus                    | The car is in front of the house               | The car is standing in front of the house .          |\n",
        "| 3   | Die Katze liegt auf dem Sofa.                  | The cat is lying on the sofa.                  | The cat is laying on the couch .                      |\n",
        "| 4   | Ein Tischler fängt einen Wettbewerb für eine Aufgabe. | A carpenter starts a competition for a task. | There is a male athlete catching an open task for an task . |\n",
        "| 5   | Eine Gruppe von Personen steht vor einem Labor. | A group of people stand in front of a laboratory. | A group of people stand in front of a lab .         |\n",
        "| 6   | Der freien Himmel scheint hell erleuchteten Himmel auf den klaren Himmel. | The open sky shines brightly lit sky on the clear sky. | A professional sky - deep lit sky in the clear blue sky . |\n",
        "| 7   | Kinder spielen im Park .                       | Children play in the park.                     | There are kids playing in the park .                  |\n"
      ],
      "metadata": {
        "id": "UoCztaU5CE1l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The BLEU scores suggest that the translation model performs poorly on some sentences (e.g., sentences 3, 4, 6, and 7) while performing relatively better on others (e.g., sentences 1, 2, and 5)."
      ],
      "metadata": {
        "id": "thbl-6e-Gqur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the above model\n",
        "\n",
        "torch.save(transformer.state_dict(), \"modelGE.pt\")\n"
      ],
      "metadata": {
        "id": "uisYTRXAyuhZ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model\n",
        "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
        "transformer.load_state_dict(torch.load(\"modelGE.pt\"))"
      ],
      "metadata": {
        "id": "Ef0QOKHIGQQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can improve the model performance by:\n",
        "1. Training the model on a larger and more diverse dataset, which can help it learn a wider range of language patterns, idiomatic expressions, and domain-specific terminology.\n",
        "2. Augmenting the training data with techniques like back-translation, where sentences are translated back and forth between the source and target languages, can help expose the model to more diverse language patterns and improve its robustness\n",
        "3. Applying regularization techniques such as dropout, weight decay, or layer normalization during training can prevent overfitting and improve the generalization ability of the model."
      ],
      "metadata": {
        "id": "Y-31vQLKHBfb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-qLMDVYRHfo9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}